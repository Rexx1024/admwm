{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1f35ec-f848-49d6-b088-de54e732b67b",
   "metadata": {},
   "source": [
    "Install required python packages"
   ]
  },
  {
   "cell_type": "code",
   "id": "2a13875b-7d01-4041-99c7-dff0eaeec60e",
   "metadata": {},
   "source": "!pip install datasets==2.19.1 langchain-community==0.2.0 langchain-chroma==0.1.1 sentence-transformers==2.7.0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6295a7e3ed4eca8c",
   "metadata": {},
   "source": [
    "Some imports we need to run the RAG demonstration."
   ]
  },
  {
   "cell_type": "code",
   "id": "e0fa019e58ddbedc",
   "metadata": {},
   "source": [
    "import chromadb\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.vectorstores import VectorStore"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d12f0b099401c294",
   "metadata": {},
   "source": [
    "Code to ignore warnings. Not a good code practice but fine for the demo."
   ]
  },
  {
   "cell_type": "code",
   "id": "ab98a4c7b398b674",
   "metadata": {},
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fc3bb770d030e08",
   "metadata": {},
   "source": [
    "The generator should generate an answer based on the user query and the relevant documents.\n",
    "We introduce a abstract class to work with generators and implementing a PromptGenerator.\n",
    "The PromptGenerator is only creating a prompt which can be executed with any LLM you like."
   ]
  },
  {
   "cell_type": "code",
   "id": "eafa9949cf1b7cd",
   "metadata": {},
   "source": [
    "class Generator:\n",
    "    def invoke(self, query: str, documents: List[Document]) -> str:\n",
    "        pass\n",
    "\n",
    "class PromptGenerator(Generator):\n",
    "    def __init__(self):\n",
    "        template = (\"Use only the provided information following after \\\"Context:\\\" to answer the question following after \\\"Question:\\\" at the end.\\n\" +\n",
    "                    \"If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\" +\n",
    "                    \"Use three sentences maximum and keep the answer as concise as possible.\\n\\n\" +\n",
    "                    \"Context: {context}\\n\\n\" +\n",
    "                    \"Question: {question}\")\n",
    "        self.prompt_template = PromptTemplate.from_template(template)\n",
    "        \n",
    "    def invoke(self, query: str, documents: List[Document]) -> str:\n",
    "        context = \"\\n\".join([f\"{i+1}. {doc.page_content}\" for i, doc in enumerate(documents)])\n",
    "        prompt = self.prompt_template.format(question=query, context=context)\n",
    "        \n",
    "        return prompt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "df543da062d2bb4f",
   "metadata": {},
   "source": [
    "We are not implementing the Retriever because there is already an implementation available in LangChain.\n",
    "Instead, we will use that implementation, and we will wrap the creation of the retriever behind a Builder.\n",
    "The Builder will also implement some logic to enhance the existing retriever with some new knowledge."
   ]
  },
  {
   "cell_type": "code",
   "id": "7e6a487bbd392fe",
   "metadata": {},
   "source": [
    "class Builder:\n",
    "    @staticmethod\n",
    "    def create_retriever(docs: Optional[List[Document]]) -> VectorStore:\n",
    "        collection_name=\"my_doc_store\"\n",
    "        embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        \n",
    "        if docs:\n",
    "            db = Chroma.from_documents(\n",
    "                documents=docs,\n",
    "                collection_name=collection_name,\n",
    "                embedding=embedding_function,\n",
    "            )\n",
    "        else:\n",
    "            db = Chroma(\n",
    "                client=chromadb.Client(),\n",
    "                collection_name=collection_name,\n",
    "                embedding_function=embedding_function,\n",
    "            )\n",
    "        \n",
    "        return db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"k\": 5, \"score_threshold\": 0.5})\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_knowledge(retriever: VectorStore, knowledge_collection: List[str]):\n",
    "        retriever.add_documents([Document(page_content=knowledge) for knowledge in knowledge_collection])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "866a0fa275acaa3",
   "metadata": {},
   "source": [
    "To chain the retriever and generator together to implement our RAG we can make use of the Orchestrator.\n",
    "Honestly, I don't know if there is a way to implement this with some LangChain pipeline, but I think for demonstration purpose that is enough. "
   ]
  },
  {
   "cell_type": "code",
   "id": "af6515490cd80bfe",
   "metadata": {},
   "source": [
    "class Orchestrator:\n",
    "    def __init__(self, retriever: VectorStore, generator: Generator):\n",
    "        self.retriever = retriever\n",
    "        self.generator = generator\n",
    "        \n",
    "    def answer_question(self, question: str) -> str:\n",
    "        relevant_documents = self.retriever.invoke(question)\n",
    "        return self.generator.invoke(question, relevant_documents)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c7257b75ddf0c366",
   "metadata": {},
   "source": [
    "Now that we have all of our components let's initialize our RAG with some data about the seven wonders"
   ]
  },
  {
   "cell_type": "code",
   "id": "35067d899fbd0bfa",
   "metadata": {},
   "source": [
    "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
    "docs = [Document(page_content=doc[\"content\"], meta=doc[\"meta\"]) for doc in dataset]\n",
    "\n",
    "retriever = Builder.create_retriever(docs)\n",
    "generator = PromptGenerator()\n",
    "\n",
    "rag = Orchestrator(retriever, generator)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "21770abac65c5a80",
   "metadata": {},
   "source": [
    "Let's see what our RAG is returning"
   ]
  },
  {
   "cell_type": "code",
   "id": "45c28450daf9dbd0",
   "metadata": {},
   "source": [
    "print(rag.answer_question(\"What happened to the Tomb of Mausolus?\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c09c57818a7a8c8e",
   "metadata": {},
   "source": [
    "Let's try what will happen when we ask our RAG about some stuff around our sun."
   ]
  },
  {
   "cell_type": "code",
   "id": "6b6176f9f643640a",
   "metadata": {},
   "source": [
    "print(rag.answer_question(\"How hot is the sun?\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ad604aaa18388039",
   "metadata": {},
   "source": [
    "Obviously our RAG can't give us any answer about the sun because it only includes some knowledge about the seven wonders.\n",
    "That's a little bit sad when we want to know something about the sun but also very great because we don't see any hallucinated answer.\n",
    "But we still want to get some answers about the sun so let's add some new knowledge!"
   ]
  },
  {
   "cell_type": "code",
   "id": "73eb509439753ffd",
   "metadata": {},
   "source": [
    "Builder.add_knowledge(retriever, [\"The sun is a star located at the center of our solar system.\", \"The sun is extremely hot!\", \"The sun has a very high temperature.\"])\n",
    "print(rag.answer_question(\"How hot is the sun?\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b36a04dd3332c6c0",
   "metadata": {},
   "source": [
    "Nice now we are able to find the new knowledge that the sun is really hot.\n",
    "To finish our demonstration we can also show you how important document chunking is and what impact it can have to get the knowledge we want.\n",
    "We will add some information about the sun's core temperature, but we will add some additional information which is not related to the temperature of the sun."
   ]
  },
  {
   "cell_type": "code",
   "id": "477837fe5e8d9b8",
   "metadata": {},
   "source": [
    "Builder.add_knowledge(retriever, [\"Here is some text to hide some interesting and useful information. The cores temperature of the sun core is approximately 15 million degrees Celsius. That should demonstrate why and how important document chunking is.\"])\n",
    "print(rag.answer_question(\"How hot is the sun?\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aa0d3c78236d35e7",
   "metadata": {},
   "source": "We will not find the information about the temperature of the sun even when it is available in our knowledge base."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
