{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": "# Retrieval-Augmented Generation (RAG) Demonstration"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Some imports we need to run the RAG demonstration.",
   "id": "6295a7e3ed4eca8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:42:46.678985Z",
     "start_time": "2024-05-20T14:42:46.677544Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e0fa019e58ddbedc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Code to ignore warnings. Not a good code practice but fine for the demo.",
   "id": "d12f0b099401c294"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:42:46.719393Z",
     "start_time": "2024-05-20T14:42:46.712793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "ab98a4c7b398b674",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The generator should generate an answer based on the user query and the relevant documents.\n",
    "We introduce a abstract class to work with generators and implementing a PromptGenerator.\n",
    "The PromptGenerator is only creating a prompt which can be executed with any LLM you like."
   ],
   "id": "fc3bb770d030e08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:42:47.018108Z",
     "start_time": "2024-05-20T14:42:46.720664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "class Generator:\n",
    "    def invoke(self, query: str, documents: List[Document]) -> str:\n",
    "        pass\n",
    "\n",
    "class PromptGenerator(Generator):\n",
    "    def __init__(self):\n",
    "        template = (\"Use only the provided information following after \\\"Context:\\\" to answer the question following after \\\"Question:\\\" at the end.\\n\" +\n",
    "                    \"If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\" +\n",
    "                    \"Use three sentences maximum and keep the answer as concise as possible.\\n\\n\" +\n",
    "                    \"Context: {context}\\n\\n\" +\n",
    "                    \"Question: {question}\")\n",
    "        self.prompt_template = PromptTemplate.from_template(template)\n",
    "        \n",
    "    def invoke(self, query: str, documents: List[Document]) -> str:\n",
    "        context = \"\\n\".join([f\"{i+1}. {doc.page_content}\" for i, doc in enumerate(documents)])\n",
    "        prompt = self.prompt_template.format(question=query, context=context)\n",
    "        \n",
    "        return prompt"
   ],
   "id": "eafa9949cf1b7cd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We are not implementing the Retriever because there is already an implementation available in LangChain.\n",
    "Instead, we will use that implementation, and we will wrap the creation of the retriever behind a Builder.\n",
    "The Builder will also implement some logic to enhance the existing retriever with some new knowledge."
   ],
   "id": "df543da062d2bb4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:42:47.549349Z",
     "start_time": "2024-05-20T14:42:47.019127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "from typing import Optional\n",
    "from langchain_core.documents.base import Document\n",
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "\n",
    "class Builder:\n",
    "    @staticmethod\n",
    "    def create_retriever(docs: Optional[List[Document]]) -> VectorStore:\n",
    "        collection_name=\"my_doc_store\"\n",
    "        embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "        \n",
    "        if docs:\n",
    "            db = Chroma.from_documents(\n",
    "                documents=docs,\n",
    "                collection_name=collection_name,\n",
    "                embedding=embedding_function,\n",
    "            )\n",
    "        else:\n",
    "            db = Chroma(\n",
    "                client=chromadb.Client(),\n",
    "                collection_name=collection_name,\n",
    "                embedding_function=embedding_function,\n",
    "            )\n",
    "        \n",
    "        return db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"k\": 5, \"score_threshold\": 0.5})\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_knowledge(retriever: VectorStore, knowledge_collection: List[str]):\n",
    "        retriever.add_documents([Document(page_content=knowledge) for knowledge in knowledge_collection])"
   ],
   "id": "7e6a487bbd392fe",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To chain the retriever and generator together to implement our RAG we can make use of the Orchestrator.\n",
    "Honestly, I don't know if there is a way to implement this with some LangChain pipeline, but I think for demonstration purpose that is enough. "
   ],
   "id": "866a0fa275acaa3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:42:47.553020Z",
     "start_time": "2024-05-20T14:42:47.550317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.vectorstores import VectorStore\n",
    "\n",
    "class Orchestrator:\n",
    "    def __init__(self, retriever: VectorStore, generator: Generator):\n",
    "        self.retriever = retriever\n",
    "        self.generator = generator\n",
    "        \n",
    "    def answer_question(self, question: str) -> str:\n",
    "        relevant_documents = self.retriever.invoke(question)\n",
    "        return self.generator.invoke(question, relevant_documents)"
   ],
   "id": "af6515490cd80bfe",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now that we have all of our components let's initialize our RAG with some data about the seven wonders",
   "id": "c7257b75ddf0c366"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:43:00.740541Z",
     "start_time": "2024-05-20T14:42:47.554442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
    "docs = [Document(page_content=doc[\"content\"], meta=doc[\"meta\"]) for doc in dataset]\n",
    "\n",
    "retriever = Builder.create_retriever(docs)\n",
    "generator = PromptGenerator()\n",
    "\n",
    "rag = Orchestrator(retriever, generator)"
   ],
   "id": "35067d899fbd0bfa",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's see what our RAG is returning",
   "id": "21770abac65c5a80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:43:00.759540Z",
     "start_time": "2024-05-20T14:43:00.741384Z"
    }
   },
   "cell_type": "code",
   "source": "print(rag.answer_question(\"What happened to the Tomb of Mausolus?\"))",
   "id": "45c28450daf9dbd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use only the provided information following after \"Context:\" to answer the question following after \"Question:\" at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "Use three sentences maximum and keep the answer as concise as possible.\n",
      "\n",
      "Context: 1. Mausolus decided to build a new capital, one as safe from capture as it was magnificent to be seen. He chose the city of Halicarnassus. Artemisia and Mausolus spent huge amounts of tax money to embellish the city. They commissioned statues, temples and buildings of gleaming marble. In 353Â BC, Mausolus died, leaving Artemisia to rule alone. As the Persian satrap, and as the Hecatomnid dynast, Mausolus had planned for himself an elaborate tomb. When he died the project was continued by his siblings. The tomb became so famous that Mausolus's name is now the eponym for all stately tombs, in the word mausoleum.[citation needed]\n",
      "Artemisia lived for only two years after the death of her husband. The urns with their ashes were placed in the yet unfinished tomb. As a form of sacrifice, the bodies of a large number of dead animals were placed on the stairs leading to the tomb, and then the stairs were filled with stones and rubble, sealing the access. \n",
      "2. [citation needed]\n",
      "During the fortification work, a party of knights entered the base of the monument and discovered the room containing a great coffin. In many histories of the Mausoleum one can find the following story of what happened: the party, deciding it was too late to open it that day, returned the next morning to find the tomb, and any treasure it may have contained, plundered. The bodies of Mausolus and Artemisia were missing too. The small museum building next to the site of the Mausoleum tells the story. Research done by archeologists in the 1960s shows that long before the knights came, grave robbers had dug a tunnel under the grave chamber, stealing its contents. Also the museum states that it is most likely that Mausolus and Artemisia were cremated, so only an urn with their ashes was placed in the grave chamber. This explains why no bodies were found.[citation needed]\n",
      "Before grinding and burning much of the remaining sculpture of the Mausoleum into lime for plaster, the Knights removed several of the best works and mounted them in the Bodrum castle. There they stayed for three centuries.\n",
      "\n",
      "Question: What happened to the Tomb of Mausolus?\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's try what will happen when we ask our RAG about some stuff around our sun.",
   "id": "c09c57818a7a8c8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:43:00.774705Z",
     "start_time": "2024-05-20T14:43:00.760557Z"
    }
   },
   "cell_type": "code",
   "source": "print(rag.answer_question(\"How hot is the sun?\"))",
   "id": "6b6176f9f643640a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use only the provided information following after \"Context:\" to answer the question following after \"Question:\" at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "Use three sentences maximum and keep the answer as concise as possible.\n",
      "\n",
      "Context: \n",
      "\n",
      "Question: How hot is the sun?\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Obviously our RAG can't give us any answer about the sun because it only includes some knowledge about the seven wonders.\n",
    "That's a little bit sad when we want to know something about the sun but also very great because we don't see any hallucinated answer.\n",
    "But we still want to get some answers about the sun so let's add some new knowledge!"
   ],
   "id": "ad604aaa18388039"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:43:00.802624Z",
     "start_time": "2024-05-20T14:43:00.775659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Builder.add_knowledge(retriever, [\"The sun is a star located at the center of our solar system.\", \"The sun is extremely hot!\", \"The sun has a very high temperature.\"])\n",
    "print(rag.answer_question(\"How hot is the sun?\"))"
   ],
   "id": "73eb509439753ffd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use only the provided information following after \"Context:\" to answer the question following after \"Question:\" at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "Use three sentences maximum and keep the answer as concise as possible.\n",
      "\n",
      "Context: 1. The sun is extremely hot!\n",
      "2. The sun has a very high temperature.\n",
      "\n",
      "Question: How hot is the sun?\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nice now we are able to find the new knowledge that the sun is really hot.\n",
    "To finish our demonstration we can also show you how important document chunking is and what impact it can have to get the knowledge we want.\n",
    "We will add some information about the sun's core temperature, but we will add some additional information which is not related to the temperature of the sun."
   ],
   "id": "b36a04dd3332c6c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T14:43:00.831204Z",
     "start_time": "2024-05-20T14:43:00.803632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Builder.add_knowledge(retriever, [\"Here is some text to hide some interesting and useful information. The cores temperature of the sun core is approximately 15 million degrees Celsius. That should demonstrate why and how important document chunking is.\"])\n",
    "print(rag.answer_question(\"How hot is the sun?\"))"
   ],
   "id": "477837fe5e8d9b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use only the provided information following after \"Context:\" to answer the question following after \"Question:\" at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "Use three sentences maximum and keep the answer as concise as possible.\n",
      "\n",
      "Context: 1. The sun is extremely hot!\n",
      "2. The sun has a very high temperature.\n",
      "\n",
      "Question: How hot is the sun?\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Damn we will not find the information about the temperature of the sun even when it is available in our knowledge base.",
   "id": "aa0d3c78236d35e7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:admwm] *",
   "language": "python",
   "name": "conda-env-admwm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
